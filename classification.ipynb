{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T08:11:37.567719Z",
     "start_time": "2018-12-15T08:11:36.609906Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, pdb, cv2, random, datetime, shutil, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from pprint import pprint, pformat\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataloaders.Classification_Image import OCT_image_classification, OCT_image_multi_classification\n",
    "from dataloaders.Image_transforms import Resize, Split_h, Normalize_divide, Random_flip, Rotation, Translation, Shear    \n",
    "\n",
    "import utils, metrics\n",
    "from networks.classification.ResNet import ResNet34,ResNet50,ResNet101\n",
    "from networks.classification.ResNet_original import ResNet18_original, ResNet34_original, ResNet50_original\n",
    "from networks.classification.DenseNet_original import DenseNet121_original, DenseNet169_original\n",
    "from utils import aic_fundus_lesion_classification\n",
    "\n",
    "def log_best_metric(metric_list, cur_epoch_idx, logger, state, save_path, save_model=True, metric = \"AUC\"):\n",
    "    if len(metric_list) == 0:\n",
    "        return\n",
    "    else:\n",
    "        if save_model:\n",
    "            dir_path = os.path.dirname(save_path)  # get parent path\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "            torch.save(state, save_path)\n",
    "            logger.info(\"Model saved in file: %s\"%(save_path))\n",
    "        best_idx = np.argmax(metric_list)\n",
    "        best_metric = metric_list[best_idx]\n",
    "        if best_idx == cur_epoch_idx:\n",
    "            logger.info(\"Epoch: %d, Validation %s improved to %.4f\"%(cur_epoch_idx, metric, best_metric))\n",
    "        else:\n",
    "            logger.info(\"Epoch: %d, Validation %s didn't improve. Best is %.4f in epoch %d\"%(cur_epoch_idx, metric, best_metric, best_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T08:11:37.593126Z",
     "start_time": "2018-12-15T08:11:37.572324Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.multiple_label = {\"name\": \"multiple_label\",\n",
    "                        \"included_pixels\": [0, 255, 191, 128],\n",
    "                        \"label_dict\": OrderedDict([(255, 0), (191, 1), (128, 2)]), # the value represent the position of 1. OrderedDict([(255, 0), (191, 1), (128, 2)])\n",
    "                        \"aug_label_dict\": None}\n",
    "                        \n",
    "        self.batch_size = 64\n",
    "        self.num_classes = 3\n",
    "        \n",
    "        self.num_split = 3\n",
    "        self.start_h = 50\n",
    "        \n",
    "        self.target_h = 224\n",
    "        self.target_w = 224\n",
    "        \n",
    "        self.task = self.multiple_label\n",
    "        self.network = \"DenseNet169_original\"\n",
    "        self.net_config = None\n",
    "        self.lr = 1.0\n",
    "        self.suffix = \"aug_multipleLabel3\" # aug_oversample_includeNormal\n",
    "        self.checkpoint = None\n",
    "        \n",
    "        self.gpus = \"3\"\n",
    "        self.num_workers = 1\n",
    "        self.nepoch = 200\n",
    "        self.manualSeed = None\n",
    "        \n",
    "\n",
    "config = Config()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T08:11:20.738249Z",
     "start_time": "2018-12-15T08:11:20.704504Z"
    }
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('logs', config.task['name'], config.network, '{}.log'.format(config.suffix))\n",
    "if os.path.exists(log_path):\n",
    "    delete_log = raw_input(\"The log file %s exist, delete it or not (y/n) \\n\"%(log_path))\n",
    "    if delete_log in ['y', 'Y']:\n",
    "        os.remove(log_path)\n",
    "    else:\n",
    "        log_path = os.path.join('logs', config.task['name'], config.network, '{}_{}.log'.format(config.suffix, datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "\n",
    "checkpoint_path = os.path.join('checkpoint', config.task['name'], config.network, config.suffix)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    delete_checkpoint_path = raw_input(\"The checkpoint folder %s exist, delete it or not (y/n) \\n\"%(checkpoint_path))\n",
    "    if delete_checkpoint_path in ['y', 'Y']:\n",
    "        shutil.rmtree(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(\"checkpoint\", config.task['name'], config.network, config.suffix+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "else:\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "summary_path = os.path.join(\"summaries\", config.task['name'], config.network, config.suffix)\n",
    "if os.path.exists(summary_path):\n",
    "    delete_summary = raw_input(\"The tf_summary folder %s exist, delete it or not (y/n) \\n\"%(summary_path))\n",
    "    if del\n",
    "    ete_summary in ['y', 'Y']:\n",
    "        shutil.rmtree(summary_path)\n",
    "    else:\n",
    "        summary_path = os.path.join(\"summaries\", config.task['name'], config.network, config.suffix+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "else:\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "\n",
    "logger = utils.get_logger(log_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "logger.info(config.__dict__)\n",
    "\n",
    "if config.manualSeed is None:\n",
    "    config.manualSeed = random.randint(1, 10000)\n",
    "logger.info(\"Random Seed: {}\".format(config.manualSeed))\n",
    "np.random.seed(config.manualSeed)\n",
    "random.seed(config.manualSeed)\n",
    "torch.manual_seed(config.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T08:12:44.964130Z",
     "start_time": "2018-12-15T08:11:39.049850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:54<00:00,  1.28it/s]\n",
      "100%|██████████| 15/15 [00:11<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tr = transforms.Compose([\n",
    "                     Resize((config.target_h, config.target_w)),\n",
    "                     Random_flip(1),\n",
    "                     Rotation(20),\n",
    "                     Translation(50),\n",
    "                     Shear(2),\n",
    "                     Normalize_divide(255.0)])\n",
    "val_tr = transforms.Compose([\n",
    "                     Resize((config.target_h, config.target_w)),\n",
    "                     Normalize_divide(255.0)])\n",
    "    \n",
    "\n",
    "trainset = OCT_image_multi_classification(\"./data/Edema_trainingset/original_images\", \n",
    "                     \"./data/Edema_trainingset/label_images\",\n",
    "                     included_pixels = config.task[\"included_pixels\"],\n",
    "                     label_dict = config.task[\"label_dict\"], \n",
    "                     aug_dict = config.task[\"aug_label_dict\"],\n",
    "                     num_classes = config.num_classes,\n",
    "                     transform = train_tr)\n",
    "\n",
    "valset = OCT_image_multi_classification(\"./data/Edema_validationset/original_images\", \n",
    "                     \"./data/Edema_validationset/label_images\",\n",
    "                     included_pixels = config.task[\"included_pixels\"], \n",
    "                     label_dict = config.task[\"label_dict\"], \n",
    "                     aug_dict = None,\n",
    "                     num_classes = config.num_classes,\n",
    "                     transform = val_tr)\n",
    "\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size = config.batch_size,\n",
    "                                         shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "valset_loader = torch.utils.data.DataLoader(valset, batch_size = config.batch_size,\n",
    "                                         shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T08:13:12.542449Z",
     "start_time": "2018-12-15T08:13:12.518202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4659., 2929.,  401.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = trainset.labels\n",
    "# normal_count = 0\n",
    "# for label in labels:\n",
    "#     if np.sum(label) == 0:\n",
    "#         normal_count += 1\n",
    "# print(normal_count)\n",
    "np.sum(np.stack(labels), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T09:59:34.188529Z",
     "start_time": "2018-09-20T09:59:34.064989Z"
    }
   },
   "outputs": [],
   "source": [
    "'''get recall, precision and f1\n",
    "'''\n",
    "def statistic(pred_logits, y_logits, label_size=2):\n",
    "    confusion_mat = np.zeros((label_size, label_size))\n",
    "    for i, sample in enumerate(pred_logits):\n",
    "        confusion_mat[y_logits[i], pred_logits[i]] += 1\n",
    "    c = metrics.Confusion(confusion_mat)\n",
    "    acc = c.accuracy()\n",
    "    recall_list = c.recall()\n",
    "    precision_list = c.precision()\n",
    "    f1_list = c.f1()      \n",
    "    return confusion_mat, recall_list, precision_list, f1_list\n",
    "\n",
    "def train(model, device, data_loader, criterion, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    epoch_loss = []\n",
    "    with tqdm(len(data_loader)) as pbar:\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loader):\n",
    "            inputs = inputs.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.detach(), 1)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            correct += torch.sum(predicted.detach() == labels.detach())\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\"Epoch %d, Batch %d/%d, Train loss: %.4f, Train acc: %.4f\"%(epoch, \n",
    "                                                                                               batch_idx+1, len(data_loader), \n",
    "                                                                                               np.mean(epoch_loss), float(correct)/total))\n",
    "    accuracy = float(correct) / float(total)\n",
    "    loss = np.mean(epoch_loss)\n",
    "    writer.add_scalar('train/epoch_accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('train/epoch_loss', loss, epoch)\n",
    "    return accuracy, loss\n",
    "\n",
    "def validate(model, device, data_loader, num_classes, criterion, epoch, writer):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    epoch_loss, epoch_logits, y_logits, outputs_softmax = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(data_loader)):\n",
    "            inputs = inputs.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            outputs_softmax = outputs_softmax + [softmax[1] for softmax in F.softmax(outputs, dim=1).detach().cpu().numpy()]\n",
    "            \n",
    "            epoch_loss.append(loss.detach())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += torch.sum(predicted == labels)\n",
    "\n",
    "            epoch_logits = epoch_logits + [idx for idx in predicted]\n",
    "            y_logits = y_logits + [idx for idx in labels]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true = y_logits, y_score = outputs_softmax)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    confusion_mat, recall, precision, f1 = statistic(epoch_logits, y_logits, num_classes)\n",
    "    logger.info(\"Confusion matrix: \\n %s \\n Recall: %s \\n Precision: %s \\n F1: %s, \\n AUC: %.4f\"%(str(confusion_mat), \\\n",
    "                                                                        str(recall),str(precision), str(f1), float(auc_value)))\n",
    "    accuracy = float(correct) / float(total)\n",
    "    loss = np.mean(epoch_loss)\n",
    "    \n",
    "    writer.add_scalar('validation/epoch_accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('validation/epoch_loss', loss, epoch)\n",
    "    \n",
    "    return accuracy, auc_value, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T10:00:00.131261Z",
     "start_time": "2018-09-20T09:59:34.192518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = globals()[config.network](config.net_config, config.num_classes)\n",
    "\n",
    "if config.checkpoint != None and config.checkpoint != \"\":\n",
    "    logger.info(\"load checkpoint: {}\".format(config.checkpoint))\n",
    "    model.load_state_dict(torch.load(config.checkpoint))\n",
    "\n",
    "gpus = map(int, config.gpus.split(\",\"))\n",
    "if len(gpus) > 1:\n",
    "    model = nn.DataParallel(model, gpus)\n",
    "device = torch.device(\"cuda:{}\".format(gpus[0]))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T10:00:02.015414Z",
     "start_time": "2018-09-20T10:00:00.169497Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_list = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = config.lr)\n",
    "for epoch in range(config.nepoch):\n",
    "    train_acc, train_loss = train(model, device, trainset_loader, criterion, optimizer, epoch, writer)\n",
    "    logger.info(\"Epoch: %d, Train Loss: %.4f, Train Acc: %.4f\"%(epoch, train_loss, train_acc))\n",
    "    val_acc, val_auc, val_loss = validate(model, device, valset_loader, int(config.num_classes), criterion, epoch, writer)\n",
    "    metric_list.append(val_auc)\n",
    "    logger.info(\"Epoch: %d, Validation Loss: %.4f, Validation Acc: %.4f, Validation AUC: %.4f\"%(epoch, val_loss, val_acc, val_auc))\n",
    "    \n",
    "    log_best_metric(metric_list, epoch, logger, \n",
    "                  model.state_dict(),\n",
    "                  \"{}/epoch{}.pth\".format(checkpoint_path, epoch),\n",
    "                  save_model=True,\n",
    "                  metric = \"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
