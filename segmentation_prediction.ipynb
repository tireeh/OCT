{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:07:04.970907Z",
     "start_time": "2018-10-07T07:07:03.973958Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-82738cc636c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "import os, sys, pdb, shutil, random, math, cv2, datetime, cPickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "from dataloaders.Classification_Image import OCT_image_classification, OCT_classification_persample\n",
    "from dataloaders.Segmentation_Image import OCT_image_segmentation, OCT_segmentation_persample, natural_keys\n",
    "import dataloaders.Segmentation_test_transforms as test_tr\n",
    "from networks.segmentation.deeplab_xception import DeepLabv3_plus_xception\n",
    "from networks.segmentation.deeplab_resnet import DeepLabv3_plus_resnet\n",
    "\n",
    "import dataloaders.Image_transforms as Image_tr\n",
    "from networks.classification.ResNet_original import ResNet34_original, ResNet50_original, ResNet18_original\n",
    "from networks.classification.DenseNet_original import DenseNet121_original\n",
    "\n",
    "from dataloaders.Image_utils import decode_segmap, decode_segmap_sequence\n",
    "from utils import aic_fundus_lesion_segmentation\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_pickle(obj, save_path): \n",
    "    parent_path = os.path.dirname(save_path)  # get parent path\n",
    "    if not os.path.exists(parent_path):\n",
    "        os.makedirs(parent_path)\n",
    "    cPickle.dump(obj, open(save_path, \"wb\"), True)\n",
    "\n",
    "def load_pickle(pickle_path):\n",
    "    with open(pickle_path, 'rb') as fo:\n",
    "        pickle_dict = cPickle.load(fo)\n",
    "    return pickle_dict    \n",
    "    \n",
    "def load_model(network, backbone, checkpoint_path, device, gpus, os, n_classes = 4):\n",
    "    model = globals()[network](nInputChannels=1, n_classes=n_classes, os=os, backbone=backbone, checkpoint=None, ignore_prefixs = [])\n",
    "    print(\"Load %s\"%(checkpoint_path))\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "    if \"state_dict\" in checkpoint.keys():\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"module.\", \"\") # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    if len(gpus) > 1:\n",
    "        model = nn.DataParallel(model, gpus)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_labels(label_sample_path, num_samples=128):\n",
    "    # sort the image dir in numerical ascend order\n",
    "    label_images = []\n",
    "    image_names = os.listdir(label_sample_path)\n",
    "    image_names.sort(key=natural_keys)\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        label_image = cv2.imread(os.path.join(label_sample_path, image_name))[:,:,0]\n",
    "        label_images.append(label_image)\n",
    "    return label_images\n",
    "\n",
    "def aug_batch_inputs(batch_inputs, scale, is_flip):\n",
    "    \"\"\"augment a batch of image\"\"\"\n",
    "    assert len(batch_inputs.shape) == 4\n",
    "    batch_size, original_w, original_h = batch_inputs.shape[0], batch_inputs.shape[-1], batch_inputs.shape[-2]\n",
    "    target_w, target_h = int(original_w*scale), int(original_h*scale)\n",
    "    batch_images = np.squeeze(batch_inputs, 1) # remove the channel dimension\n",
    "    auged_images = np.empty((batch_size, target_h, target_w))\n",
    "    for image_idx in range(batch_size):\n",
    "        image = batch_images[image_idx]\n",
    "        image = cv2.resize(image, (target_w, target_h), interpolation = cv2.INTER_LINEAR)\n",
    "        if is_flip:\n",
    "            image = cv2.flip(image, 1)\n",
    "        auged_images[image_idx] = image\n",
    "    return auged_images\n",
    "    \n",
    "def aug_restore(batch_outputs, target_w, target_h, is_flip):\n",
    "    \"\"\"restore the augmented output to target width and height\"\"\"\n",
    "    assert len(batch_outputs.shape) == 4\n",
    "    batch_size, num_channel = batch_outputs.shape[0], batch_outputs.shape[1]\n",
    "    restored_outputs = np.empty((batch_size, num_channel, target_h, target_w))\n",
    "    for image_idx in range(batch_size):\n",
    "        image = np.transpose(batch_outputs[image_idx], [1, 2, 0])\n",
    "        image = cv2.resize(image, (target_w, target_h), interpolation = cv2.INTER_LINEAR)\n",
    "        if is_flip:\n",
    "            image = cv2.flip(image, 1)\n",
    "        restored_outputs[image_idx] = np.transpose(image, [2, 0, 1])\n",
    "    return restored_outputs\n",
    "\n",
    "def seg_predict_ensemble(models, inputs, device, test_scales, is_flip):\n",
    "    with torch.no_grad():\n",
    "        original_w, original_h = inputs.size(-1), inputs.size(-2)\n",
    "        outputs = []\n",
    "        to_flip = [True, False] if is_flip else [False]\n",
    "        inputs_arr = inputs.detach().cpu().numpy()\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            for scale in test_scales:\n",
    "                for flip in to_flip:\n",
    "                    aug_images = aug_batch_inputs(inputs_arr, scale, flip)\n",
    "                    aug_images = np.expand_dims(aug_images, 1)# add the channel dimension\n",
    "                    aug_images = torch.from_numpy(aug_images).float().to(device)\n",
    "                    aug_outputs = model(aug_images)\n",
    "                    aug_outputs = F.softmax(aug_outputs, dim=1)\n",
    "                    restored_outputs = aug_restore(aug_outputs.detach().cpu().numpy(), original_w, original_h, flip)\n",
    "                    outputs.append(restored_outputs)\n",
    "        ensemble = np.mean(np.stack(outputs), 0)\n",
    "    return ensemble\n",
    "        \n",
    "def seg_main(models, dataloader, device, test_scales, is_flip):\n",
    "    samples, sample_predictions = [], []\n",
    "    for inputs in dataloader:\n",
    "        output_ensemble = seg_predict_ensemble(models, inputs, device, test_scales, is_flip)\n",
    "        predictions = np.argmax(output_ensemble, 1)\n",
    "        samples.append(inputs.cpu().numpy())\n",
    "        sample_predictions.append(predictions)\n",
    "    return np.concatenate(samples, 0), np.concatenate(sample_predictions, 0)\n",
    "\n",
    "def write_disk(target_root, sample_names, sample_predictions):\n",
    "    for i in range(len(sample_names)):\n",
    "        target_path = os.path.join(target_root, sample_names[i]+\"_volumes.npy\")\n",
    "        np.save(target_path, sample_predictions[i].astype(\"uint8\")) # save as uint8 to save space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:07:04.999850Z",
     "start_time": "2018-10-07T07:07:04.975184Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_classification_models(device, gpus):\n",
    "    def load_classification_models(model_config, num_classes, device, gpus):\n",
    "        model = globals()[model_config.network](model_config.net_config, num_classes)\n",
    "        print(\"Load %s\"%(model_config.checkpoint))\n",
    "        model.load_state_dict(torch.load(model_config.checkpoint))\n",
    "        if len(gpus) > 1:\n",
    "            model = nn.DataParallel(model, gpus)\n",
    "        model.to(device)\n",
    "        return model\n",
    "    \n",
    "    model_config = namedtuple(\"Model\", [\"network\", \"checkpoint\", \"net_config\"])\n",
    "    model_configs = [\n",
    "                 model_config(\"ResNet50_original\", \"checkpoint/multiple_label/ResNet50_original/aug_multipleLabel/epoch102.pth\", None),\n",
    "                 model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel/epoch56.pth\", None),\n",
    "                model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel3/epoch2.pth\", None),\n",
    "                model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel2/epoch1.pth\", None),\n",
    "                model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel/epoch0.pth\", None),\n",
    "                \n",
    "                model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel/epoch8.pth\", None),\n",
    "                model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel2/epoch162.pth\", None),\n",
    "                model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel3/epoch141.pth\", None),\n",
    "                \n",
    "                model_config(\"ResNet50_original\", \"checkpoint/multiple_label/ResNet50_original/aug_multipleLabel2/epoch17.pth\", None),\n",
    "                model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel4/epoch29.pth\", None),\n",
    "                model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel5/epoch20.pth\", None),\n",
    "                ]\n",
    "    return [load_classification_models(model_configObj, 3, device, gpus) for model_configObj in model_configs]\n",
    "\n",
    "def classification_prediction(inputs, models, device):\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.float()\n",
    "        inputs = inputs.to(device)\n",
    "        softmaxs = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            output = model(inputs)\n",
    "            softmax = torch.sigmoid(output).detach().cpu().numpy()\n",
    "            softmaxs.append(softmax)\n",
    "        ensemble = np.mean(softmaxs, 0)\n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:07:05.109496Z",
     "start_time": "2018-10-07T07:07:05.003494Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_abnormal(image_arr, targets):\n",
    "    counts = [np.count_nonzero(image_arr == target) for target in targets]\n",
    "    return counts\n",
    "\n",
    "def divide_blocks(image_predictions, target_pixel, num_threshold):\n",
    "    \"\"\"divide the prediction of images in an sample by counting the number of target pixel\"\"\"\n",
    "    blocks, blocks_numpixel = [], []\n",
    "    start_idx, end_idx = -1, -1\n",
    "    for i, prediction in enumerate(image_predictions):\n",
    "        num_target_pixel = np.count_nonzero(prediction == target_pixel)\n",
    "        if num_target_pixel >= num_threshold:\n",
    "            if start_idx == -1: # start a new block\n",
    "                start_idx = i\n",
    "                end_idx = -1\n",
    "        else:\n",
    "            if start_idx > -1: # the block is supposed to be ended\n",
    "                end_idx = i # end of a block \n",
    "                if (end_idx - 1) == start_idx: # the block contains only one image\n",
    "                    blocks.append([start_idx])\n",
    "                else:\n",
    "                    blocks.append([start_idx, end_idx - 1])\n",
    "                start_idx = -1 # restart a new block\n",
    "                end_idx = -1\n",
    "        if i == (len(image_predictions) - 1): # only the last image is into consideration\n",
    "            if start_idx > -1 and start_idx == i:\n",
    "                blocks.append([i])\n",
    "            elif start_idx > -1 and start_idx != i:\n",
    "                blocks.append([start_idx, i])\n",
    "    return blocks\n",
    "\n",
    "def count_pixels(blocks, image_predictions, target_pixel):\n",
    "    \"\"\"count number of target pixel in blocks\"\"\"\n",
    "    blocks_numpixel = []\n",
    "    for block in blocks:\n",
    "        assert len(block) in [1, 2]\n",
    "        if len(block) == 1:\n",
    "            num_pixels = np.count_nonzero(image_predictions[block[0]] == target_pixel)\n",
    "            blocks_numpixel.append(num_pixels)\n",
    "        else:\n",
    "            start_idx, end_idx = block[0], block[1]\n",
    "            num_pixels = np.sum([np.count_nonzero(image_predictions[i] == target_pixel) for i in range(start_idx, end_idx + 1)])\n",
    "            blocks_numpixel.append(num_pixels)\n",
    "    return blocks_numpixel\n",
    "\n",
    "def remove_block_connection(classification_index, classification_dataloader, classification_models, device,\n",
    "                     image_predictions,\n",
    "                     blocks, block_numpixels,\n",
    "                     class_type, target_class_type = 0,\n",
    "                     abnormal_threshold = 0.5, passed_percent = 0.5):\n",
    "    for block_idx, block in enumerate(blocks):\n",
    "        images_passed = []\n",
    "        if len(block) == 1:\n",
    "            start_idx, end_idx = block[0], block[0]\n",
    "        else:\n",
    "            start_idx, end_idx = block[0], block[1]\n",
    "            \n",
    "        for inputs_idx, inputs in enumerate(classification_dataloader):\n",
    "            if inputs_idx >= start_idx and inputs_idx <= end_idx:\n",
    "                classification_softmax = classification_prediction(inputs, classification_models, device)\n",
    "                if classification_softmax[0, classification_index] >= abnormal_threshold:\n",
    "                    images_passed.append(1)\n",
    "                else:\n",
    "                    images_passed.append(0)\n",
    "        \n",
    "        if float(np.sum(images_passed)) / len(images_passed) < passed_percent: # replace the class type\n",
    "            if block_idx == np.argmax(block_numpixels):\n",
    "                print(\"Segmentation class %d, Total %d blocks, %d-th block has %d valid images, %d abnormal pixels (largest block), ignore !!!\"%(class_type, len(blocks),block_idx, (end_idx - start_idx + 1), block_numpixels[block_idx]))\n",
    "#                 print(\"Segmentation class %d, Total %d blocks, Remove %d-th block has %d valid images, %d abnormal pixels (largest block) !!!\"%(class_type, len(blocks),block_idx, (end_idx - start_idx + 1), block_numpixels[block_idx]))\n",
    "            else:\n",
    "                print(\"Segmentation class %d, Total %d blocks, Remove %d-th block has %d valid images, %d abnormal pixels\"%(class_type, len(blocks), block_idx, (end_idx - start_idx + 1), block_numpixels[block_idx]))\n",
    "                for i in range(start_idx, end_idx + 1):\n",
    "                    np.place(image_predictions[i], image_predictions[i]==class_type, target_class_type)\n",
    "    return image_predictions    \n",
    "\n",
    "def replace_pixel(sample_predictions, source_pixel, target_pixel):\n",
    "    \"\"\"replace source pixel with target pixel in sample prediction \"\"\"\n",
    "    replaced_predictions = []\n",
    "    for sample_prediction in sample_predictions:\n",
    "        np.place(sample_prediction, sample_prediction==source_pixel, target_pixel)\n",
    "        replaced_predictions.append(sample_prediction)\n",
    "    return replaced_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:07:05.200441Z",
     "start_time": "2018-10-07T07:07:05.113882Z"
    }
   },
   "outputs": [],
   "source": [
    "def vanilla_seg(root_path, seg_models, seg_tr, device, batch_size, num_workers, test_scales, is_flip):\n",
    "    sample_images, sample_predictions, sample_names = [], [], []\n",
    "    for sample_name in tqdm(sorted(os.listdir(root_path))):\n",
    "        sample_names.append(sample_name.replace(\".img\", \"\"))\n",
    "        sample_path = os.path.join(root_path, sample_name)\n",
    "        seg_dataset = OCT_segmentation_persample(sample_path, transform = seg_tr)\n",
    "        seg_datasetloader = torch.utils.data.DataLoader(seg_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
    "        images, predictions = seg_main(seg_models, seg_datasetloader, device, test_scales, is_flip)\n",
    "        sample_images.append(images)\n",
    "        sample_predictions.append(predictions)\n",
    "    return sample_images, sample_predictions, sample_names\n",
    "\n",
    "def seg_connection(root_path, seg_models, classification_models, seg_tr, classification_tr, \n",
    "                   device, seg_batch_size, num_workers, class_included, class_thresholds, abnormal_threshold, passed_percent,\n",
    "                   test_scales, is_flip):\n",
    "    sample_images, sample_predictions, sample_names = [], [], []\n",
    "    for sample_idx, sample_name in enumerate(sorted(os.listdir(root_path))):\n",
    "        time_start = datetime.datetime.now()\n",
    "        sample_names.append(sample_name.replace(\".img\", \"\"))\n",
    "        sample_path = os.path.join(root_path, sample_name)\n",
    "        seg_dataset = OCT_segmentation_persample(sample_path, transform = seg_tr)\n",
    "        seg_datasetloader = torch.utils.data.DataLoader(seg_dataset, batch_size = seg_batch_size, shuffle=False, num_workers=num_workers)\n",
    "        images, predictions = seg_main(seg_models, seg_datasetloader, device, test_scales, is_flip)\n",
    "        \n",
    "        classification_dataset = OCT_classification_persample(sample_path, classification_tr)\n",
    "        classification_loader = torch.utils.data.DataLoader(classification_dataset, batch_size = 1,\n",
    "                                             shuffle=False, num_workers=config.num_workers)\n",
    "        \n",
    "        for class_type in class_included:\n",
    "            classification_index = class_type - 1\n",
    "            blocks = divide_blocks(predictions, class_type, class_thresholds[class_type])\n",
    "            block_numpixels = count_pixels(blocks, predictions, class_type)\n",
    "            predictions = remove_block_connection(classification_index, classification_loader, classification_models, device,\n",
    "                         predictions,\n",
    "                         blocks, block_numpixels,\n",
    "                         class_type, target_class_type = 0,\n",
    "                         abnormal_threshold=abnormal_threshold, passed_percent=passed_percent)\n",
    "        time_end = datetime.datetime.now()\n",
    "        sample_images.append(images)\n",
    "        sample_predictions.append(predictions)\n",
    "        print(\"{}-th sample processed, cost {} seconds----------------------------\".format(sample_idx, (time_end-time_start).seconds))\n",
    "    return sample_images, sample_predictions, sample_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T07:07:05.376125Z",
     "start_time": "2018-10-07T07:07:05.363644Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-f86d3b1bdcc2>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-f86d3b1bdcc2>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    self.batch_size = 10\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.label_dict = OrderedDict([(0, 0), (255, 1), (191, 2), (128, 3)])\n",
    "        self.gpus = \"0, 1, 2, 3\"\n",
    "        self.os = 16\n",
    "        self.n_classes = 4\n",
    "        self.batch_size = 20\n",
    "        \n",
    "        self.num_workers = 6\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:01:55.919584Z",
     "start_time": "2018-10-07T07:07:05.781436Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpus = map(int, config.gpus.split(\",\"))\n",
    "device = torch.device(\"cuda:{}\".format(gpus[0]))\n",
    "\n",
    "# ResNet101, checkpoint/segmentation/DeepLabv3_plus_resnet/aug_512_1024/epoch13.pth\n",
    "# ResNet50, checkpoint/segmentation/DeepLabv3_plus_resnet/aug_classweight_1_1_1.5_10/epoch34.pth\n",
    "# ResNet50, checkpoint/segmentation/DeepLabv3_plus_resnet/aug_dice_scale_0.75_1.5_weight_1_1.5_1.5_6/epoch31.pth\n",
    "\n",
    "seg_models = [load_model(\"DeepLabv3_plus_resnet\", \"ResNet101\", \"checkpoint/segmentation/DeepLabv3_plus_resnet/aug_ResNet101_cross_entropy_avg1_scale_0.5_2.0_weight_1_1.5_1.5_10/epoch12.pth\", \n",
    "               device, gpus, 16, config.n_classes)]\n",
    "\n",
    "seg_tr = transforms.Compose([\n",
    "        test_tr.Normalize_divide(255.0),\n",
    "        test_tr.ToTensor()])\n",
    "\n",
    "classification_models = get_classification_models(device, gpus)\n",
    "classification_tr = transforms.Compose([\n",
    "                             Image_tr.Resize((224, 224)),\n",
    "                             Image_tr.Normalize_divide(255.0)])\n",
    "\n",
    "root_path = \"./data/Edema_testset/original_images\"\n",
    "\n",
    "sample_images, sample_predictions, sample_names = vanilla_seg(root_path, seg_models, seg_tr, device, \n",
    "                                                              batch_size=config.batch_size, num_workers=config.num_workers,\n",
    "                                                              test_scales = [0.5, 0.75, 1.0, 1.25, 1.50, 1.75], is_flip = True)\n",
    "\n",
    "# sample_images, sample_predictions, sample_names = seg_connection(root_path, seg_models, classification_models, \n",
    "#                                                                  seg_tr, classification_tr, \n",
    "#                                                                  device, config.batch_size, config.num_workers,\n",
    "#                                                                  class_included = [1, 2],\n",
    "#                                                                  class_thresholds = {1: 500, 2: 10, 3:5},\n",
    "#                                                                  abnormal_threshold=0.75, passed_percent=0.5,\n",
    "#                                                                  test_scales = [0.5, 0.75, 1.0, 1.25, 1.50, 1.75], is_flip = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T08:05:38.472471Z",
     "start_time": "2018-10-07T08:05:26.032570Z"
    }
   },
   "outputs": [],
   "source": [
    "save_pickle(sample_names, \"./predictions/pickles/20181007/sample_names_epoch12.pkl\")\n",
    "save_pickle(sample_predictions, \"./predictions/pickles/20181007/segmentation_sample_predictions_epoch12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T04:49:01.548920Z",
     "start_time": "2018-10-07T04:48:52.436633Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_names = load_pickle(\"./predictions/pickles/20181007/sample_names_epoch12.pkl\")\n",
    "sample_predictions = load_pickle(\"./predictions/pickles/20181007/segmentation_sample_predictions_epoch12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:49:52.806081Z",
     "start_time": "2018-10-07T06:49:48.526054Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_predictions = replace_pixel(sample_predictions, 2, 0)\n",
    "sample_predictions = replace_pixel(sample_predictions, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T06:49:55.689622Z",
     "start_time": "2018-10-07T06:49:53.622543Z"
    }
   },
   "outputs": [],
   "source": [
    "write_disk(\"./predictions/segmentation/test/xxy/20181007_1/\", sample_names, sample_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T03:15:18.314257Z",
     "start_time": "2018-10-07T03:14:30.391661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vallabel_root = \"./data/Edema_validationset/label_images\"\n",
    "sample_labels = [get_labels(os.path.join(vallabel_root, sample_name)) for sample_name in tqdm(sorted(os.listdir(vallabel_root)))]\n",
    "sample_dices = []\n",
    "for sample_idx in range(len(sample_labels)):\n",
    "    label = np.array(sample_labels[sample_idx])\n",
    "    for target_pixel in config.label_dict:\n",
    "        np.place(label, label==target_pixel, config.label_dict[target_pixel])\n",
    "    \n",
    "    prediction = np.array(sample_predictions[sample_idx])\n",
    "    sample_dice = aic_fundus_lesion_segmentation(label, prediction)\n",
    "    sample_dices.append(sample_dice)\n",
    "    print(sample_idx, sample_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-07T03:15:22.121365Z",
     "start_time": "2018-10-07T03:15:22.110386Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dices = [[], [], [], []]\n",
    "for sample_idx in range(len(sample_dices)):\n",
    "    for data_type in range(4):\n",
    "        dice_score = sample_dices[sample_idx][data_type]\n",
    "        if not math.isnan(dice_score):\n",
    "            valid_dices[data_type].append(dice_score)\n",
    "print(\"mean class dices: %s\"%([np.mean(dices) for dices in valid_dices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d4b95dc6170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_labels' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T07:33:36.835714Z",
     "start_time": "2018-10-02T07:33:05.926944Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_idx = 13\n",
    "for image_idx in range(128):\n",
    "    image = np.squeeze(sample_images[sample_idx][image_idx])\n",
    "    label = sample_labels[sample_idx][image_idx]\n",
    "    prediction = sample_predictions[sample_idx][image_idx]\n",
    "    plt.figure()\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(label, cmap = \"gray\")\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(decode_segmap(prediction), cmap = \"gray\")\n",
    "    plt.show()\n",
    "    print(\"%d, label   abnormal pixels: %s\"%(image_idx, count_abnormal(label, [255, 191, 128])))\n",
    "    print(\"%d, predict abnormal pixels: %s\"%(image_idx, count_abnormal(prediction, [1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T06:20:51.490707Z",
     "start_time": "2018-09-28T06:20:51.411041Z"
    }
   },
   "outputs": [],
   "source": [
    "class_thresholds = {1: 500, 2: 10, 3:5}\n",
    "class_type = 1\n",
    "\n",
    "rea_blocks = divide_blocks(sample_predictions[sample_idx], class_type, class_thresholds[class_type])\n",
    "block_numpixels = count_pixels(rea_blocks, sample_predictions[sample_idx], class_type)\n",
    "print(rea_blocks)\n",
    "print(block_numpixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T02:14:47.942247Z",
     "start_time": "2018-09-28T01:57:45.441Z"
    }
   },
   "outputs": [],
   "source": [
    "# original dice scores per sample, DeepLab V3+, aug_512_1024, epoch 13\n",
    "# 0, [0.9953913572955032, 0.8340105323339809, 0.897754417707465, nan]\n",
    "# 1, [0.9711029532694698, 0.5825124021131763, nan, nan]\n",
    "# 2, [0.984575074181537, 0.87036935702214, nan, nan]\n",
    "# 3, [0.9804807003555078, 0.7994834911793456, 0.49694016868922786, nan]\n",
    "# 4, [0.978341909341848, 0.835843416784337, 0.5977946941352797, nan]\n",
    "# 5, [0.9954415541089505, 0.8556120417099563, nan, nan]\n",
    "# 6, [0.9978480137764166, 0.6011353655545492, nan, 0.7816266971196548]\n",
    "# 7, [0.9810973092135972, 0.7503458588967595, 0.5657404002731523, nan]\n",
    "# 8, [0.9738636436197384, 0.6333658331661316, nan, nan]\n",
    "# 9, [0.9960554992443774, 0.8708277615796239, 0.9339293126129993, nan]\n",
    "# 10, [0.9968853684332056, 0.9228649151460038, 0.9291939554231939, 0.06471816283924843]\n",
    "# 11, [0.9970407295810517, 0.918798821803022, 0.9196883861597178, 0.6642642642642642]\n",
    "# 12, [0.9979058149555472, 0.9435037670132433, 0.9506723939470507, 0.6248153618906942]\n",
    "# 13, [0.9996719836525478, 0.6989183047530227, nan, 0.7698178237321517]\n",
    "# 14, [0.9994861342220099, 0.6812009412102815, nan, 0.7936418359668924]\n",
    "# mean class dices: [0.9896792030167539, 0.7865861873510382, 0.7864642161185108, 0.6164806909688175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T02:14:47.944034Z",
     "start_time": "2018-09-28T01:57:45.443Z"
    }
   },
   "outputs": [],
   "source": [
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.5, passed_percent: 0.5\n",
    "# [0.9899642521913241, 0.7941926848197653, 0.7969739480984086, 0.616742385984297]\n",
    "\n",
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.6, passed_percent: 0.5\n",
    "# [0.9901855736327988, 0.7961173806069044, 0.7969738600277836, 0.6045752257517375]\n",
    "\n",
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.4, passed_percent: 0.5\n",
    "# [0.9899414191899925, 0.7934372731537295, 0.7969738600277836, 0.616742385984297]\n",
    "\n",
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.4, passed_percent: 0.4\n",
    "# [0.9899414191899925, 0.7934372731537295, 0.7969738600277836, 0.616742385984297]\n",
    "\n",
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.7, passed_percent: 0.5, classes: [1, 2]\n",
    "# [0.9902553894011289, 0.7967099289736387, 0.7969738600277836, 0.6164806909688175]\n",
    "\n",
    "# {1: 800, 2: 10, 3:5}, abnormal threshold: 0.75, passed_percent: 0.5, classes: [1, 2]\n",
    "# [0.9904760316560257, 0.7977752882140663, 0.7969738600277836, 0.6164806909688175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T03:27:35.199476Z",
     "start_time": "2018-10-15T03:27:35.187382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.9902149958506881, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
