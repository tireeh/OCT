{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:49:46.734473Z",
     "start_time": "2018-10-24T06:49:39.991781Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, pdb, shutil, random, math, datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dataloaders import Segmentation_transforms as tr\n",
    "from dataloaders.Segmentation_Image import OCT_image_segmentation\n",
    "from networks.segmentation.deeplab_xception import DeepLabv3_plus_xception\n",
    "from networks.segmentation.deeplab_resnet import DeepLabv3_plus_resnet\n",
    "from networks.segmentation.deeplab_resnet_random import DeepLabv3_plus_resnet_random\n",
    "from networks.segmentation.coordconv_unet import UNet_coordconv\n",
    "from networks.segmentation.coordconv_unet_sn_elu import UNet_coordconv_sn_elu\n",
    "\n",
    "from dataloaders.Image_utils import decode_segmap, decode_segmap_sequence\n",
    "from utils import get_logger, get_dice_score, lr_poly, aic_fundus_lesion_segmentation\n",
    "from losses import CrossEntropy2D, DiceLoss2D\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:49:50.357449Z",
     "start_time": "2018-10-24T06:49:50.315837Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.train_batch = 8 # ResNet101: 20, ResNet50: 28, ResNet34: 44, UNet: 20\n",
    "        self.val_batch = 7\n",
    "        \n",
    "        self.nepoch = 50\n",
    "        \n",
    "        self.h = 1024 # original is 1024\n",
    "        self.w = 512\n",
    "        self.lr = 1.0*1e-2 # 1.0*1e-7\n",
    "        self.num_classes = 4\n",
    "        self.class_weight = [1, 1.5, 1.5, 10] # [1, 1.5, 1.5, 6]\n",
    "        self.wd = 5e-4\n",
    "        self.momentum = 0.9\n",
    "        self.nAveGrad = 1\n",
    "         \n",
    "        self.dataset = \"Edema\" # Edema | defined1 | defined2\n",
    "        self.network = \"DeepLabv3_plus_resnet\"\n",
    "        self.net_config = {\"keep_probs\": [0.8]*4}\n",
    "        self.os = 16\n",
    "        self.backbone = \"ResNet50\" # ResNet34 | ResNet50 | ResNet101\n",
    "        self.pretrain_checkpoint = \"./pretrained/resnet50.pth\" #/root/.torch/models/resnet50-19c8e357.pth | checkpoint/edema_PED/ResNet101_pretrain/aug/epoch7.pth\n",
    "        self.ignore_prefixs = [\"conv1\", \"fc\", \"gap\"] # [\"conv1\", \"fc\", \"gap\"]\n",
    "        self.criterion = \"cross_entropy\" # cross_entropy | dice\n",
    "        self.scale_min = 0.75\n",
    "        self.scale_max = 1.5\n",
    "        self.rotation = 15\n",
    "        self.denoising = False\n",
    "        \n",
    "        self.task = \"segmentation\"\n",
    "        self.suffix = \"Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01\"\n",
    "        self.checkpoint = None\n",
    "        self.included_pixels = [0, 255, 191, 128]\n",
    "        self.label_dict = OrderedDict([(0, 0), (255, 1), (191, 2), (128, 3)]) #OrderedDict([(0, 0), (255, 1), (191, 2), (128, 3)])\n",
    "        self.aug_dict = None #OrderedDict([(128, 7)])\n",
    "        \n",
    "        self.gpus = \"0, 1\"\n",
    "        self.num_workers =2\n",
    "        \n",
    "        self.manualSeed = None\n",
    "        \n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:49:51.442524Z",
     "start_time": "2018-10-24T06:49:51.155952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The checkpoint folder checkpoint/segmentation/DeepLabv3_plus_resnet/Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01 exist, delete it or not (y/n) \n",
      "y\n",
      "The tf_summary folder summaries/segmentation/DeepLabv3_plus_resnet/Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01 exist, delete it or not (y/n) \n",
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'train_batch': 8, 'val_batch': 7, 'nepoch': 50, 'h': 1024, 'w': 512, 'lr': 0.01, 'num_classes': 4, 'class_weight': [1, 1.5, 1.5, 10], 'wd': 0.0005, 'momentum': 0.9, 'nAveGrad': 1, 'dataset': 'Edema', 'network': 'DeepLabv3_plus_resnet', 'net_config': {'keep_probs': [0.8, 0.8, 0.8, 0.8]}, 'os': 16, 'backbone': 'ResNet50', 'pretrain_checkpoint': './pretrained/resnet50.pth', 'ignore_prefixs': ['conv1', 'fc', 'gap'], 'criterion': 'cross_entropy', 'scale_min': 0.75, 'scale_max': 1.5, 'rotation': 15, 'denoising': False, 'task': 'segmentation', 'suffix': 'Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01', 'checkpoint': None, 'included_pixels': [0, 255, 191, 128], 'label_dict': OrderedDict([(0, 0), (255, 1), (191, 2), (128, 3)]), 'aug_dict': None, 'gpus': '0, 1', 'num_workers': 2, 'manualSeed': None}\n",
      "Random Seed: 4312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb56a5592d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join('logs', config.task, config.network, '{}.log'.format(config.suffix))\n",
    "if os.path.exists(log_path):\n",
    "    delete_log = input(\"The log file %s exist, delete it or not (y/n) \\n\"%(log_path))\n",
    "    if delete_log in ['y', 'Y']:\n",
    "        os.remove(log_path)\n",
    "    else:\n",
    "        log_path = os.path.join('logs', config.task, config.network, '{}_{}.log'.format(config.suffix, datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "\n",
    "checkpoint_path = os.path.join('checkpoint', config.task, config.network, config.suffix)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    delete_checkpoint_path = input(\"The checkpoint folder %s exist, delete it or not (y/n) \\n\"%(checkpoint_path))\n",
    "    if delete_checkpoint_path in ['y', 'Y']:\n",
    "        shutil.rmtree(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(\"checkpoint\", config.task, config.network, config.suffix+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "else:\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "summary_path = os.path.join(\"summaries\", config.task, config.network, config.suffix)\n",
    "if os.path.exists(summary_path):\n",
    "    delete_summary = input(\"The tf_summary folder %s exist, delete it or not (y/n) \\n\"%(summary_path))\n",
    "    if delete_summary in ['y', 'Y']:\n",
    "        shutil.rmtree(summary_path)\n",
    "    else:\n",
    "        summary_path = os.path.join(\"summaries\", config.task, config.network, config.suffix+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "else:\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "logger = get_logger(log_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "logger.info(config.__dict__)\n",
    "\n",
    "if config.manualSeed is None:\n",
    "    config.manualSeed = random.randint(1, 10000)\n",
    "logger.info(\"Random Seed: {}\".format(config.manualSeed))\n",
    "np.random.seed(config.manualSeed)\n",
    "random.seed(config.manualSeed)\n",
    "torch.manual_seed(config.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:49:56.258735Z",
     "start_time": "2018-10-24T06:49:56.083324Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_best_metric(metric_list, cur_epoch_idx, logger, state, save_path, save_model=True, metric = \"AUC\"):\n",
    "    if len(metric_list) == 0:\n",
    "        return\n",
    "    else:\n",
    "        best_idx = np.argmax(metric_list)\n",
    "        best_metric = metric_list[best_idx]\n",
    "        if best_idx == cur_epoch_idx:\n",
    "            logger.info(\"Epoch: %d, Validation %s improved to %.4f\"%(cur_epoch_idx, metric, best_metric))\n",
    "            if save_model:\n",
    "                dir_path = os.path.dirname(save_path)  # get parent path\n",
    "                if not os.path.exists(dir_path):\n",
    "                    os.makedirs(dir_path)\n",
    "                torch.save(state, save_path)\n",
    "                logger.info(\"Model saved in file: %s\"%(save_path))\n",
    "        else:\n",
    "            logger.info(\"Epoch: %d, Validation %s didn't improve. Best is %.4f in epoch %d\"%(cur_epoch_idx, metric, best_metric, best_idx))\n",
    "\n",
    "def train(model, device, data_loader, criterion, nAveGrad, optimizer, epoch, writer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    aveGrad = 0\n",
    "    with tqdm(len(data_loader)) as pbar:\n",
    "        for batch_idx, sample_batched in enumerate(data_loader):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "            inputs = inputs.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            n_classes = outputs.size(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss /= nAveGrad # TODO why loss needed divided by the nAveGrad ?\n",
    "            loss.backward()\n",
    "            aveGrad += 1\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if aveGrad % nAveGrad == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                aveGrad = 0\n",
    "                \n",
    "            pbar.update(1)\n",
    "            pbar.set_description(\"Epoch %d, Batch %d/%d, Train loss: %.4f\"%(epoch, batch_idx+1, len(data_loader), np.mean(losses)))\n",
    "    \n",
    "    ave_loss = np.mean(losses)\n",
    "    input_images = vutils.make_grid(inputs, padding = 5, normalize=False)\n",
    "    gt_idxs = np.expand_dims(labels.detach().cpu().numpy(), 1)\n",
    "    gt_images = vutils.make_grid(decode_segmap_sequence(gt_idxs), padding = 5, normalize=False, range=(0, 255))\n",
    "    predicted_idxs = np.expand_dims(torch.max(outputs, 1)[1].detach().cpu().numpy(), 1)\n",
    "    predicted_images = vutils.make_grid(decode_segmap_sequence(predicted_idxs), padding = 5, normalize=False, range=(0, 255))\n",
    "    \n",
    "    writer.add_image('train/input_images', input_images, epoch)\n",
    "    writer.add_image('train/ground_truth', gt_images, epoch)\n",
    "    writer.add_image('train/predictions', predicted_images, epoch)\n",
    "    writer.add_scalar('train/epoch_loss', ave_loss, epoch)\n",
    "    return ave_loss\n",
    "\n",
    "def validate(model, device, data_loader, criterion, epoch, writer):\n",
    "    losses, sample_predictions, sample_ground_truth = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample_batched in enumerate(tqdm(data_loader)):\n",
    "            inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "            inputs = inputs.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.max(outputs, 1)[1]\n",
    "            sample_predictions.append(predictions.cpu().numpy().astype(np.int16))\n",
    "\n",
    "            n_classes = outputs.size(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            sample_ground_truth.append(labels.cpu().numpy().astype(np.int16))\n",
    "        \n",
    "        # only write the last batch in testset \n",
    "        input_images = vutils.make_grid(inputs, padding = 5, normalize=False)\n",
    "        gt_idxs = np.expand_dims(labels.detach().cpu().numpy(), 1)\n",
    "        gt_images = vutils.make_grid(decode_segmap_sequence(gt_idxs), padding = 5, normalize=False, range=(0, 255))\n",
    "        predicted_idxs = np.expand_dims(torch.max(outputs, 1)[1].detach().cpu().numpy(), 1)\n",
    "        predicted_images = vutils.make_grid(decode_segmap_sequence(predicted_idxs), padding = 5, normalize=False, range=(0, 255))\n",
    "\n",
    "        writer.add_image('test/input_images', input_images, epoch)\n",
    "        writer.add_image('test/ground_truth', gt_images, epoch)\n",
    "        writer.add_image('test/predictions', predicted_images, epoch)\n",
    "    \n",
    "    ave_loss = np.mean(losses)\n",
    "    writer.add_scalar('test/epoch_loss', ave_loss, epoch)\n",
    "    return ave_loss, np.concatenate(sample_predictions, 0), np.squeeze(np.concatenate(sample_ground_truth, 0))\n",
    "\n",
    "def mean_dice_persample(all_outputs, all_labels, num_image = 128):\n",
    "    sample_dices = []\n",
    "    outputs, labels = [], []\n",
    "    for i in range(len(all_outputs)):\n",
    "        outputs.append(all_outputs[i])\n",
    "        labels.append(all_labels[i])\n",
    "        if (i+1) % 128 == 0:\n",
    "            sample_dices.append(aic_fundus_lesion_segmentation(np.array(labels), np.array(outputs)))\n",
    "            outputs, labels = [], []\n",
    "    valid_dices = [[], [], [], []]\n",
    "    for sample_dice in sample_dices:\n",
    "        for i, dice_value in enumerate(sample_dice):\n",
    "            if not math.isnan(dice_value):\n",
    "                valid_dices[i].append(dice_value)\n",
    "    return [round(np.mean(dice_values), 5) for dice_values in valid_dices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:53:23.198362Z",
     "start_time": "2018-10-24T06:49:56.838856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:17<00:00,  3.93it/s]\n",
      "100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "train_tr = transforms.Compose([\n",
    "        tr.RandomHorizontalFlip(),\n",
    "        tr.RandomSized([config.h, config.w], config.scale_min, config.scale_max), # h, w\n",
    "        tr.RandomRotate(config.rotation),\n",
    "        tr.Normalize_divide(255.0),\n",
    "        tr.ToTensor()]) \n",
    "\n",
    "val_tr = transforms.Compose([\n",
    "#         tr.FixedResize([config.h, config.w]), # h, w\n",
    "        tr.Normalize_divide(255.0),\n",
    "        tr.ToTensor()])\n",
    "\n",
    "trainset = OCT_image_segmentation(\"./data/{}_trainingset/original_images\".format(config.dataset), \n",
    "                     \"./data/{}_trainingset/label_images\".format(config.dataset),\n",
    "                     included_pixels = config.included_pixels,\n",
    "                     label_dict = config.label_dict,\n",
    "                     aug_dict = config.aug_dict,\n",
    "                     denoising = config.denoising,\n",
    "                     transform = train_tr)\n",
    "valset = OCT_image_segmentation(\"./data/{}_validationset/original_images\".format(config.dataset), \n",
    "                     \"./data/{}_validationset/label_images\".format(config.dataset),\n",
    "                     included_pixels = config.included_pixels,\n",
    "                     label_dict = config.label_dict,\n",
    "                     aug_dict = None,\n",
    "                     denoising = config.denoising,\n",
    "                     transform = val_tr)\n",
    "\n",
    "trainset_loader = DataLoader(trainset, batch_size=config.train_batch, drop_last=True,shuffle=True, num_workers=config.num_workers)\n",
    "valset_loader = DataLoader(valset, batch_size=config.val_batch, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:53:34.506633Z",
     "start_time": "2018-10-24T06:53:23.203910Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing DeepLabv3+ model...\n",
      "Number of classes: 4\n",
      "Output stride: 16\n",
      "Number of Input Channels: 1\n",
      "load bn1.running_mean\n",
      "load bn1.running_var\n",
      "load bn1.weight\n",
      "load bn1.bias\n",
      "load layer1.0.conv1.weight\n",
      "load layer1.0.bn1.running_mean\n",
      "load layer1.0.bn1.running_var\n",
      "load layer1.0.bn1.weight\n",
      "load layer1.0.bn1.bias\n",
      "load layer1.0.conv2.weight\n",
      "load layer1.0.bn2.running_mean\n",
      "load layer1.0.bn2.running_var\n",
      "load layer1.0.bn2.weight\n",
      "load layer1.0.bn2.bias\n",
      "load layer1.0.conv3.weight\n",
      "load layer1.0.bn3.running_mean\n",
      "load layer1.0.bn3.running_var\n",
      "load layer1.0.bn3.weight\n",
      "load layer1.0.bn3.bias\n",
      "load layer1.0.downsample.0.weight\n",
      "load layer1.0.downsample.1.running_mean\n",
      "load layer1.0.downsample.1.running_var\n",
      "load layer1.0.downsample.1.weight\n",
      "load layer1.0.downsample.1.bias\n",
      "load layer1.1.conv1.weight\n",
      "load layer1.1.bn1.running_mean\n",
      "load layer1.1.bn1.running_var\n",
      "load layer1.1.bn1.weight\n",
      "load layer1.1.bn1.bias\n",
      "load layer1.1.conv2.weight\n",
      "load layer1.1.bn2.running_mean\n",
      "load layer1.1.bn2.running_var\n",
      "load layer1.1.bn2.weight\n",
      "load layer1.1.bn2.bias\n",
      "load layer1.1.conv3.weight\n",
      "load layer1.1.bn3.running_mean\n",
      "load layer1.1.bn3.running_var\n",
      "load layer1.1.bn3.weight\n",
      "load layer1.1.bn3.bias\n",
      "load layer1.2.conv1.weight\n",
      "load layer1.2.bn1.running_mean\n",
      "load layer1.2.bn1.running_var\n",
      "load layer1.2.bn1.weight\n",
      "load layer1.2.bn1.bias\n",
      "load layer1.2.conv2.weight\n",
      "load layer1.2.bn2.running_mean\n",
      "load layer1.2.bn2.running_var\n",
      "load layer1.2.bn2.weight\n",
      "load layer1.2.bn2.bias\n",
      "load layer1.2.conv3.weight\n",
      "load layer1.2.bn3.running_mean\n",
      "load layer1.2.bn3.running_var\n",
      "load layer1.2.bn3.weight\n",
      "load layer1.2.bn3.bias\n",
      "load layer2.0.conv1.weight\n",
      "load layer2.0.bn1.running_mean\n",
      "load layer2.0.bn1.running_var\n",
      "load layer2.0.bn1.weight\n",
      "load layer2.0.bn1.bias\n",
      "load layer2.0.conv2.weight\n",
      "load layer2.0.bn2.running_mean\n",
      "load layer2.0.bn2.running_var\n",
      "load layer2.0.bn2.weight\n",
      "load layer2.0.bn2.bias\n",
      "load layer2.0.conv3.weight\n",
      "load layer2.0.bn3.running_mean\n",
      "load layer2.0.bn3.running_var\n",
      "load layer2.0.bn3.weight\n",
      "load layer2.0.bn3.bias\n",
      "load layer2.0.downsample.0.weight\n",
      "load layer2.0.downsample.1.running_mean\n",
      "load layer2.0.downsample.1.running_var\n",
      "load layer2.0.downsample.1.weight\n",
      "load layer2.0.downsample.1.bias\n",
      "load layer2.1.conv1.weight\n",
      "load layer2.1.bn1.running_mean\n",
      "load layer2.1.bn1.running_var\n",
      "load layer2.1.bn1.weight\n",
      "load layer2.1.bn1.bias\n",
      "load layer2.1.conv2.weight\n",
      "load layer2.1.bn2.running_mean\n",
      "load layer2.1.bn2.running_var\n",
      "load layer2.1.bn2.weight\n",
      "load layer2.1.bn2.bias\n",
      "load layer2.1.conv3.weight\n",
      "load layer2.1.bn3.running_mean\n",
      "load layer2.1.bn3.running_var\n",
      "load layer2.1.bn3.weight\n",
      "load layer2.1.bn3.bias\n",
      "load layer2.2.conv1.weight\n",
      "load layer2.2.bn1.running_mean\n",
      "load layer2.2.bn1.running_var\n",
      "load layer2.2.bn1.weight\n",
      "load layer2.2.bn1.bias\n",
      "load layer2.2.conv2.weight\n",
      "load layer2.2.bn2.running_mean\n",
      "load layer2.2.bn2.running_var\n",
      "load layer2.2.bn2.weight\n",
      "load layer2.2.bn2.bias\n",
      "load layer2.2.conv3.weight\n",
      "load layer2.2.bn3.running_mean\n",
      "load layer2.2.bn3.running_var\n",
      "load layer2.2.bn3.weight\n",
      "load layer2.2.bn3.bias\n",
      "load layer2.3.conv1.weight\n",
      "load layer2.3.bn1.running_mean\n",
      "load layer2.3.bn1.running_var\n",
      "load layer2.3.bn1.weight\n",
      "load layer2.3.bn1.bias\n",
      "load layer2.3.conv2.weight\n",
      "load layer2.3.bn2.running_mean\n",
      "load layer2.3.bn2.running_var\n",
      "load layer2.3.bn2.weight\n",
      "load layer2.3.bn2.bias\n",
      "load layer2.3.conv3.weight\n",
      "load layer2.3.bn3.running_mean\n",
      "load layer2.3.bn3.running_var\n",
      "load layer2.3.bn3.weight\n",
      "load layer2.3.bn3.bias\n",
      "load layer3.0.conv1.weight\n",
      "load layer3.0.bn1.running_mean\n",
      "load layer3.0.bn1.running_var\n",
      "load layer3.0.bn1.weight\n",
      "load layer3.0.bn1.bias\n",
      "load layer3.0.conv2.weight\n",
      "load layer3.0.bn2.running_mean\n",
      "load layer3.0.bn2.running_var\n",
      "load layer3.0.bn2.weight\n",
      "load layer3.0.bn2.bias\n",
      "load layer3.0.conv3.weight\n",
      "load layer3.0.bn3.running_mean\n",
      "load layer3.0.bn3.running_var\n",
      "load layer3.0.bn3.weight\n",
      "load layer3.0.bn3.bias\n",
      "load layer3.0.downsample.0.weight\n",
      "load layer3.0.downsample.1.running_mean\n",
      "load layer3.0.downsample.1.running_var\n",
      "load layer3.0.downsample.1.weight\n",
      "load layer3.0.downsample.1.bias\n",
      "load layer3.1.conv1.weight\n",
      "load layer3.1.bn1.running_mean\n",
      "load layer3.1.bn1.running_var\n",
      "load layer3.1.bn1.weight\n",
      "load layer3.1.bn1.bias\n",
      "load layer3.1.conv2.weight\n",
      "load layer3.1.bn2.running_mean\n",
      "load layer3.1.bn2.running_var\n",
      "load layer3.1.bn2.weight\n",
      "load layer3.1.bn2.bias\n",
      "load layer3.1.conv3.weight\n",
      "load layer3.1.bn3.running_mean\n",
      "load layer3.1.bn3.running_var\n",
      "load layer3.1.bn3.weight\n",
      "load layer3.1.bn3.bias\n",
      "load layer3.2.conv1.weight\n",
      "load layer3.2.bn1.running_mean\n",
      "load layer3.2.bn1.running_var\n",
      "load layer3.2.bn1.weight\n",
      "load layer3.2.bn1.bias\n",
      "load layer3.2.conv2.weight\n",
      "load layer3.2.bn2.running_mean\n",
      "load layer3.2.bn2.running_var\n",
      "load layer3.2.bn2.weight\n",
      "load layer3.2.bn2.bias\n",
      "load layer3.2.conv3.weight\n",
      "load layer3.2.bn3.running_mean\n",
      "load layer3.2.bn3.running_var\n",
      "load layer3.2.bn3.weight\n",
      "load layer3.2.bn3.bias\n",
      "load layer3.3.conv1.weight\n",
      "load layer3.3.bn1.running_mean\n",
      "load layer3.3.bn1.running_var\n",
      "load layer3.3.bn1.weight\n",
      "load layer3.3.bn1.bias\n",
      "load layer3.3.conv2.weight\n",
      "load layer3.3.bn2.running_mean\n",
      "load layer3.3.bn2.running_var\n",
      "load layer3.3.bn2.weight\n",
      "load layer3.3.bn2.bias\n",
      "load layer3.3.conv3.weight\n",
      "load layer3.3.bn3.running_mean\n",
      "load layer3.3.bn3.running_var\n",
      "load layer3.3.bn3.weight\n",
      "load layer3.3.bn3.bias\n",
      "load layer3.4.conv1.weight\n",
      "load layer3.4.bn1.running_mean\n",
      "load layer3.4.bn1.running_var\n",
      "load layer3.4.bn1.weight\n",
      "load layer3.4.bn1.bias\n",
      "load layer3.4.conv2.weight\n",
      "load layer3.4.bn2.running_mean\n",
      "load layer3.4.bn2.running_var\n",
      "load layer3.4.bn2.weight\n",
      "load layer3.4.bn2.bias\n",
      "load layer3.4.conv3.weight\n",
      "load layer3.4.bn3.running_mean\n",
      "load layer3.4.bn3.running_var\n",
      "load layer3.4.bn3.weight\n",
      "load layer3.4.bn3.bias\n",
      "load layer3.5.conv1.weight\n",
      "load layer3.5.bn1.running_mean\n",
      "load layer3.5.bn1.running_var\n",
      "load layer3.5.bn1.weight\n",
      "load layer3.5.bn1.bias\n",
      "load layer3.5.conv2.weight\n",
      "load layer3.5.bn2.running_mean\n",
      "load layer3.5.bn2.running_var\n",
      "load layer3.5.bn2.weight\n",
      "load layer3.5.bn2.bias\n",
      "load layer3.5.conv3.weight\n",
      "load layer3.5.bn3.running_mean\n",
      "load layer3.5.bn3.running_var\n",
      "load layer3.5.bn3.weight\n",
      "load layer3.5.bn3.bias\n",
      "load layer4.0.conv1.weight\n",
      "load layer4.0.bn1.running_mean\n",
      "load layer4.0.bn1.running_var\n",
      "load layer4.0.bn1.weight\n",
      "load layer4.0.bn1.bias\n",
      "load layer4.0.conv2.weight\n",
      "load layer4.0.bn2.running_mean\n",
      "load layer4.0.bn2.running_var\n",
      "load layer4.0.bn2.weight\n",
      "load layer4.0.bn2.bias\n",
      "load layer4.0.conv3.weight\n",
      "load layer4.0.bn3.running_mean\n",
      "load layer4.0.bn3.running_var\n",
      "load layer4.0.bn3.weight\n",
      "load layer4.0.bn3.bias\n",
      "load layer4.0.downsample.0.weight\n",
      "load layer4.0.downsample.1.running_mean\n",
      "load layer4.0.downsample.1.running_var\n",
      "load layer4.0.downsample.1.weight\n",
      "load layer4.0.downsample.1.bias\n",
      "load layer4.1.conv1.weight\n",
      "load layer4.1.bn1.running_mean\n",
      "load layer4.1.bn1.running_var\n",
      "load layer4.1.bn1.weight\n",
      "load layer4.1.bn1.bias\n",
      "load layer4.1.conv2.weight\n",
      "load layer4.1.bn2.running_mean\n",
      "load layer4.1.bn2.running_var\n",
      "load layer4.1.bn2.weight\n",
      "load layer4.1.bn2.bias\n",
      "load layer4.1.conv3.weight\n",
      "load layer4.1.bn3.running_mean\n",
      "load layer4.1.bn3.running_var\n",
      "load layer4.1.bn3.weight\n",
      "load layer4.1.bn3.bias\n",
      "load layer4.2.conv1.weight\n",
      "load layer4.2.bn1.running_mean\n",
      "load layer4.2.bn1.running_var\n",
      "load layer4.2.bn1.weight\n",
      "load layer4.2.bn1.bias\n",
      "load layer4.2.conv2.weight\n",
      "load layer4.2.bn2.running_mean\n",
      "load layer4.2.bn2.running_var\n",
      "load layer4.2.bn2.weight\n",
      "load layer4.2.bn2.bias\n",
      "load layer4.2.conv3.weight\n",
      "load layer4.2.bn3.running_mean\n",
      "load layer4.2.bn3.running_var\n",
      "load layer4.2.bn3.weight\n",
      "load layer4.2.bn3.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "if config.network.startswith(\"DeepLab\"):\n",
    "    model = globals()[config.network](net_config = config.net_config, nInputChannels=1, n_classes=config.num_classes, os=config.os, \n",
    "                                      backbone=config.backbone, checkpoint=config.pretrain_checkpoint, ignore_prefixs =config.ignore_prefixs)\n",
    "elif config.network.startswith(\"UNet\"):\n",
    "    model = globals()[config.network](n_channels=1, n_classes=config.num_classes)\n",
    "else:\n",
    "    raise(\"Unknown network: {}\".format(config.network))\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.wd)\n",
    "lr_lambda = lambda epoch: (1 - float(epoch) / config.nepoch)** 0.9\n",
    "\n",
    "start_epoch = -1\n",
    "if config.checkpoint is not None:\n",
    "    checkpoint = torch.load(config.checkpoint)\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"module.\", \"\") # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    logger.info(\"Resuming from checkpoint: {} at epoch{}\".format(config.checkpoint, start_epoch))\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "    optimizer.param_groups[0][\"initial_lr\"] = config.lr # pytorch 0.4.0 need explicitly set initial_lr when resuming optimizer\n",
    "\n",
    "gpus = list(map(int, config.gpus.split(\",\")))\n",
    "\n",
    "if len(gpus) > 1:\n",
    "    model = nn.DataParallel(model, gpus)\n",
    "device = torch.device(\"cuda:{}\".format(gpus[0]))\n",
    "model.to(device)\n",
    "\n",
    "if config.criterion == \"cross_entropy\":\n",
    "    criterion = CrossEntropy2D(weight=config.class_weight, size_average=True, batch_average=True)\n",
    "elif config.criterion == \"dice\":\n",
    "    criterion = DiceLoss2D(n_classes = config.num_classes, smooth = 1)\n",
    "else:\n",
    "    raise(\"Unknown criterion: {}\".format(config.criterion))\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch = -1 if start_epoch == -1 else start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T13:29:06.665200Z",
     "start_time": "2018-10-24T06:53:34.512945Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Epoch: 0, Learning rate: 0.0098198187\n",
      "0it [00:00, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2925: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "Epoch 0, Batch 1119/1119, Train loss: 0.0749: : 1119it [09:56,  1.88it/s]\n",
      "Epoch: 0, Train Loss: 0.0749\n",
      "100%|██████████| 275/275 [00:54<00:00,  5.03it/s]\n",
      "Epoch: 0, Validation Loss: 0.1010, Validation Dice Score: 0.6840, [0.97679, 0.67323, 0.63378, 0.45208]\n",
      "Epoch: 0, Validation Dice score improved to 0.6840\n",
      "Model saved in file: checkpoint/segmentation/DeepLabv3_plus_resnet/Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01/epoch0.pth\n",
      "Epoch: 1, Learning rate: 0.0096392692\n",
      "Epoch 1, Batch 1119/1119, Train loss: 0.0431: : 1119it [10:10,  1.83it/s]\n",
      "Epoch: 1, Train Loss: 0.0431\n",
      "100%|██████████| 275/275 [00:54<00:00,  5.01it/s]\n",
      "Epoch: 1, Validation Loss: 0.0688, Validation Dice Score: 0.7433, [0.98963, 0.71918, 0.74735, 0.51696]\n",
      "Epoch: 1, Validation Dice score improved to 0.7433\n",
      "Model saved in file: checkpoint/segmentation/DeepLabv3_plus_resnet/Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01/epoch1.pth\n",
      "Epoch: 2, Learning rate: 0.0094583432\n",
      "Epoch 2, Batch 1119/1119, Train loss: 0.0382: : 1119it [10:12,  1.83it/s]\n",
      "Epoch: 2, Train Loss: 0.0382\n",
      "100%|██████████| 275/275 [00:54<00:00,  5.01it/s]\n",
      "Epoch: 2, Validation Loss: 0.0748, Validation Dice Score: 0.7077, [0.98589, 0.69408, 0.68684, 0.46385]\n",
      "Epoch: 2, Validation Dice score didn't improve. Best is 0.7433 in epoch 1\n",
      "Epoch: 3, Learning rate: 0.0092770318\n",
      "Epoch 3, Batch 1119/1119, Train loss: 0.0349: : 1119it [10:21,  1.80it/s]\n",
      "Epoch: 3, Train Loss: 0.0349\n",
      "100%|██████████| 275/275 [00:57<00:00,  4.76it/s]\n",
      "Epoch: 3, Validation Loss: 0.0775, Validation Dice Score: 0.7613, [0.98572, 0.72147, 0.72396, 0.61414]\n",
      "Epoch: 3, Validation Dice score improved to 0.7613\n",
      "Model saved in file: checkpoint/segmentation/DeepLabv3_plus_resnet/Edema_sizeAvg_aug_1024x512_cross_entropy_scale_0.75_1.5_lr_0.01/epoch3.pth\n",
      "Epoch: 4, Learning rate: 0.0090953258\n",
      "Epoch 4, Batch 625/1119, Train loss: 0.0333: : 625it [05:43,  1.82it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8329a4071772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d, Learning rate: %.10f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnAveGrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d, Train Loss: %.4f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-50c1894d0572>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, data_loader, criterion, nAveGrad, optimizer, epoch, writer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0maveGrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maveGrad\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnAveGrad\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metric_list = []\n",
    "for epoch in range(start_epoch+1, config.nepoch):\n",
    "    lr_scheduler.step()\n",
    "    logger.info(\"Epoch: %d, Learning rate: %.10f\"%(epoch, lr_scheduler.get_lr()[0]))\n",
    "    train_loss = train(model, device, trainset_loader, criterion, config.nAveGrad, optimizer, epoch, writer)\n",
    "    logger.info(\"Epoch: %d, Train Loss: %.4f\"%(epoch, train_loss))\n",
    "    \n",
    "    val_loss, val_predictions, val_ground_truths = validate(model, device, valset_loader, criterion, epoch, writer)\n",
    "    class_mean_dices = mean_dice_persample(val_predictions, val_ground_truths)\n",
    "    avg_score = np.mean(class_mean_dices)\n",
    "    metric_list.append(avg_score)\n",
    "    logger.info(\"Epoch: %d, Validation Loss: %.4f, Validation Dice Score: %.4f, %s\"%(epoch, val_loss, avg_score, class_mean_dices))\n",
    "    \n",
    "    log_best_metric(metric_list, epoch, logger, \n",
    "                    {'epoch': epoch,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'optimizer': optimizer.state_dict()},\n",
    "                     '{}/epoch{}.pth'.format(checkpoint_path, epoch),\n",
    "                    save_model=True,\n",
    "                    metric = \"Dice score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T07:20:25.924740Z",
     "start_time": "2018-10-20T07:19:35.386Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii, sample in enumerate(trainset_loader):\n",
    "    img = sample['image'].numpy()\n",
    "    gt = sample['label'].numpy()\n",
    "    for jj in range(sample[\"image\"].size(0)):\n",
    "        tmp = np.array(gt[jj]).astype(np.uint8)\n",
    "        segmap = decode_segmap(tmp, label_colours = OrderedDict([(0, 0), (1, 255), (2, 191), (3, 128)]))\n",
    "        img_tmp = np.squeeze(img[jj], 0)\n",
    "        plt.figure()\n",
    "        plt.title('display')\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img_tmp, cmap = \"gray\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(segmap, cmap = \"gray\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bc05fe6eace1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_predictions' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
