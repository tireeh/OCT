{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:14:28.881841Z",
     "start_time": "2018-10-09T10:14:27.819436Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, pdb, cv2, random, traceback, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from pprint import pprint, pformat\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, namedtuple\n",
    "from sklearn.metrics import  roc_curve, auc\n",
    "\n",
    "from dataloaders.Classification_Image import OCT_image_classification, OCT_classification_persample, natural_keys\n",
    "from dataloaders.Image_transforms import Resize, Split_h, Normalize_divide, To_CHW    \n",
    "from sklearn import metrics\n",
    "\n",
    "from networks.classification.ResNet import ResNet34, ResNet50, ResNet101\n",
    "from networks.classification.ResNet_original import ResNet34_original, ResNet50_original, ResNet18_original\n",
    "from networks.classification.DenseNet_original import DenseNet121_original, DenseNet169_original, DenseNet201_original\n",
    "\n",
    "from utils import aic_fundus_lesion_classification\n",
    "\n",
    "def save_pickle(obj, save_path): \n",
    "    parent_path = os.path.dirname(save_path)  # get parent path\n",
    "    if not os.path.exists(parent_path):\n",
    "        os.makedirs(parent_path)\n",
    "    cPickle.dump(obj, open(save_path, \"wb\"), True)\n",
    "\n",
    "def load_pickle(pickle_path):\n",
    "    with open(pickle_path, 'rb') as fo:\n",
    "        pickle_dict = cPickle.load(fo)\n",
    "    return pickle_dict\n",
    "\n",
    "def load_model(model_config, num_classes, device, gpus):\n",
    "    model = globals()[model_config.network](model_config.net_config, num_classes)\n",
    "    print(\"Load %s\"%(model_config.checkpoint))\n",
    "    model.load_state_dict(torch.load(model_config.checkpoint))\n",
    "    if len(gpus) > 1:\n",
    "        model = nn.DataParallel(model, gpus)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_labels(label_root, label_dict = OrderedDict([(255, 0), (191, 1), (128, 2)]), num_samples=128):\n",
    "    total_labels = []\n",
    "    for label_sample_name in tqdm(sorted(os.listdir(label_root))):\n",
    "        sample_labels = np.zeros((num_samples, 3))\n",
    "        # sort the image dir in numerical ascend order\n",
    "        image_names = os.listdir(os.path.join(label_root, label_sample_name))\n",
    "        image_names.sort(key=natural_keys)\n",
    "        for i, image_name in enumerate(image_names):\n",
    "            label_image = cv2.imread(os.path.join(label_root, label_sample_name, image_name))[:,:,0]\n",
    "            for target_pixel in label_dict:\n",
    "                if target_pixel in label_image:\n",
    "                    sample_labels[i, label_dict[target_pixel]] = 1\n",
    "        total_labels.append(sample_labels)\n",
    "    return total_labels\n",
    "\n",
    "def predict_ensemble(models, inputs, device, last_activation=\"sigmoid\"):\n",
    "    \"\"\"predict single input from the data loader via multiple models\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.float()\n",
    "        inputs = inputs.to(device)\n",
    "        softmaxs = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            output = model(inputs)\n",
    "            if last_activation == \"sigmoid\":\n",
    "                softmax = torch.sigmoid(output).detach().cpu().numpy()\n",
    "            elif last_activation == \"softmax\":\n",
    "                softmax = F.softmax(output, dim=1).detach().cpu().numpy()\n",
    "            else:\n",
    "                raise(\"Unknown activation function in last layer: {}\".format(last_activation))\n",
    "            softmaxs.append(softmax)\n",
    "        ensemble = np.mean(softmaxs, 0)\n",
    "    return ensemble\n",
    "\n",
    "def main(main_models,\n",
    "         REA_models,\n",
    "         SRF_models,\n",
    "         batch_size,\n",
    "         dataloader,\n",
    "         device,\n",
    "         num_classes = 2):\n",
    "    predictions = np.zeros((128, 3))\n",
    "    for batch_idx, inputs in enumerate(dataloader):\n",
    "        softmaxs = predict_ensemble(main_models, inputs, device, \"sigmoid\")\n",
    "        if REA_models is not None and len(REA_models) > 0:\n",
    "            REA_softmax = predict_ensemble(REA_models, inputs, device, \"softmax\")\n",
    "            softmaxs[:, 0] = (softmaxs[:, 0] + REA_softmax[:, 1])/2.0\n",
    "        if SRF_models is not None and len(SRF_models) > 0:\n",
    "            SRF_softmax = predict_ensemble(SRF_models, inputs, device, \"softmax\")\n",
    "            softmaxs[:, 1] = (softmaxs[:, 1] + SRF_softmax[:, 1])/2.0\n",
    "        \n",
    "        for image_idx in range(inputs.size(0)):\n",
    "            predictions[batch_idx*batch_size + image_idx, :] = softmaxs[image_idx, :]\n",
    "    return predictions\n",
    "\n",
    "def write_disk(target_root, sample_names, sample_predictions):\n",
    "    for i in range(len(sample_names)):\n",
    "        target_path = os.path.join(target_root, sample_names[i]+\"_detections.npy\")\n",
    "        np.save(target_path, sample_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:14:29.425282Z",
     "start_time": "2018-10-09T10:14:29.412354Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 240\n",
    "        self.num_classes = 3\n",
    "        \n",
    "        self.target_h = 224\n",
    "        self.target_w = 224\n",
    "        \n",
    "        self.gpus = \"0,1,2,3\"\n",
    "        self.num_workers = 6\n",
    "\n",
    "config = Config()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:15:05.069556Z",
     "start_time": "2018-10-09T10:14:30.472222Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = map(int, config.gpus.split(\",\"))\n",
    "device = torch.device(\"cuda:{}\".format(gpus[0]))\n",
    "resnet_config = {\"ensemble_idx\": 7, \"feature_size\": 7}\n",
    "\n",
    "model_config = namedtuple(\"Model\", [\"network\", \"checkpoint\", \"net_config\"])\n",
    "\n",
    "# model_config(\"ResNet50_original\", \"checkpoint/multiple_label/ResNet50_original/aug_multipleLabel/epoch102.pth\", None),\n",
    "# model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel/epoch56.pth\", None)\n",
    "\n",
    "# 1 represent AUC is improved when exclude the model\n",
    "model_configs = [\n",
    "    model_config(\"ResNet50_original\", \"checkpoint/multiple_label/ResNet50_original/aug_multipleLabel/epoch102.pth\", None),\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel/epoch56.pth\", None),\n",
    "    model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel3/epoch2.pth\", None),\n",
    "    model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel2/epoch1.pth\", None),\n",
    "    model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel/epoch0.pth\", None),\n",
    "\n",
    "    model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel/epoch8.pth\", None),\n",
    "    model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel2/epoch162.pth\", None),\n",
    "    model_config(\"ResNet18_original\", \"checkpoint/multiple_label/ResNet18_original/aug_multipleLabel3/epoch141.pth\", None),\n",
    "\n",
    "    model_config(\"ResNet50_original\", \"checkpoint/multiple_label/ResNet50_original/aug_multipleLabel2/epoch17.pth\", None),\n",
    "    model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel4/epoch29.pth\", None),\n",
    "    model_config(\"ResNet34_original\", \"checkpoint/multiple_label/ResNet34_original/aug_multipleLabel5/epoch20.pth\", None),\n",
    "\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel1/epoch23.pth\", None), #0.9863\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel2/epoch8.pth\", None), #0.9831\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel3/epoch72.pth\", None), #0.9811\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel4/epoch17.pth\", None), #0.9891\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel5/epoch112.pth\", None), #0.9859\n",
    "    model_config(\"DenseNet121_original\", \"checkpoint/multiple_label/DenseNet121_original/aug_multipleLabel6/epoch11.pth\", None), #0.9814\n",
    "    \n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel/epoch138.pth\", None), #0.9846\n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel1/epoch132.pth\", None), #0.9869\n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel2/epoch42.pth\", None), #0.9863\n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel3/epoch42.pth\", None), #0.9792\n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel4/epoch151.pth\", None), #0.9847\n",
    "    model_config(\"DenseNet169_original\", \"checkpoint/multiple_label/DenseNet169_original/aug_multipleLabel5/epoch164.pth\", None), #0.9813\n",
    "    \n",
    "    model_config(\"DenseNet201_original\", \"checkpoint/multiple_label/DenseNet201_original/aug_multipleLabel/epoch11.pth\", None), \n",
    "    model_config(\"DenseNet201_original\", \"checkpoint/multiple_label/DenseNet201_original/aug_multipleLabel2/epoch21.pth\", None), \n",
    "    \n",
    "    model_config(\"DenseNet201_original\", \"checkpoint/multiple_label/DenseNet201_original/aug_multipleLabel4/epoch78.pth\", None), \n",
    "]\n",
    "\n",
    "# REA_model_configs = [\n",
    "#     model_config(\"ResNet34_original\", \"checkpoint/normal_edema/ResNet34_original/aug/epoch23.pth\", None),\n",
    "#     model_config(\"ResNet50_original\", \"checkpoint/normal_edema/ResNet50_original/aug/epoch70.pth\", None),\n",
    "# ]\n",
    "\n",
    "# SRF_model_configs = [\n",
    "#     model_config(\"ResNet34_original\", \"checkpoint/edema_SRF/ResNet34_original/aug_oversample_includeNormal/epoch19.pth\", None),\n",
    "#     model_config(\"ResNet50_original\", \"checkpoint/edema_SRF/ResNet50_original/aug_includeNormal/epoch7.pth\", None),\n",
    "# ]\n",
    "\n",
    "main_models = [load_model(model_configObj, config.num_classes, device, gpus) for model_configObj in model_configs]\n",
    "# REA_models = [load_model(REA_config, 2, device, gpus) for REA_config in REA_model_configs]\n",
    "# SRF_models = [load_model(SRF_config, 2, device, gpus) for SRF_config in SRF_model_configs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:17:43.892905Z",
     "start_time": "2018-10-09T10:15:09.175808Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path = \"./data/Edema_testset/original_images\"\n",
    "sample_predictions, sample_names = [], []\n",
    "for sample_name in tqdm(sorted(os.listdir(root_path))):\n",
    "    sample_names.append(sample_name.replace(\".img\", \"\"))\n",
    "    sample_path = os.path.join(root_path, sample_name)\n",
    "    \n",
    "    dataset = OCT_classification_persample(sample_path, \n",
    "                         transform = transforms.Compose([\n",
    "                             Resize((config.target_h, config.target_w)),\n",
    "                             Normalize_divide(255.0)\n",
    "                         ]))\n",
    "    \n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size = config.batch_size,\n",
    "                                             shuffle=False, num_workers=config.num_workers)\n",
    "    predictions = main(main_models, \n",
    "                       None, # Note\n",
    "                       None, # Note\n",
    "                       config.batch_size,\n",
    "                       dataset_loader,\n",
    "                       device,\n",
    "                       num_classes = config.num_classes)\n",
    "    sample_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:18:13.705248Z",
     "start_time": "2018-10-09T10:18:13.692103Z"
    }
   },
   "outputs": [],
   "source": [
    "write_disk(\"./predictions/classification/20181009\", sample_names, sample_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T10:13:39.934215Z",
     "start_time": "2018-10-09T10:13:36.209892Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vallabel_root = \"./data/Edema_validationset/label_images\"\n",
    "valsample_labels = get_labels(vallabel_root)\n",
    "valsample_aucs = [aic_fundus_lesion_classification(valsample_labels[i], sample_predictions[i]) for i in range(len(sample_predictions))]\n",
    "valid_aucs = []\n",
    "valid_auc_classes = [[], [], []]\n",
    "for sample_auc in valsample_aucs:\n",
    "    for i, auc_value in enumerate(sample_auc):\n",
    "        if not math.isnan(auc_value):\n",
    "            valid_auc_classes[i].append(auc_value)\n",
    "            valid_aucs.append(auc_value)\n",
    "pprint(valsample_aucs)\n",
    "print(\"total mean auc: %.4f\"%(np.mean(valid_aucs)))\n",
    "print(\"mean class auc: %s\"%([np.mean(auc_values) for auc_values in valid_auc_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 models\n",
    "# total mean auc: 0.9840\n",
    "# mean class auc: [0.9777493510775866, 0.9956627872274264, 0.9839897245456543]\n",
    "\n",
    "# 3 models\n",
    "# total mean auc: 0.9869\n",
    "# mean class auc: [0.982373455196617, 0.9959346511605659, 0.9861977216688492]\n",
    "\n",
    "# 4 models\n",
    "# total mean auc: 0.9886\n",
    "# mean class auc: [0.9850149263734375, 0.9951161993165722, 0.9888272723487113]\n",
    "\n",
    "# 5 models, 测试提交\n",
    "# total mean auc: 0.9893\n",
    "# mean class auc: [0.9849019912306753, 0.9961495319310655, 0.9910379530745432]\n",
    "\n",
    "# 8 models\n",
    "# total mean auc: 0.9904\n",
    "# mean class auc: [0.9869663405564982, 0.9961811614857214, 0.9912466205918582]\n",
    "\n",
    "# 11 models, testset auc: 0.989\n",
    "# total mean auc: 0.9917\n",
    "# mean class auc: [0.9890446218223498, 0.9965169368214968, 0.9919791523015307]\n",
    "\n",
    "# 23 models, 0.9929\n",
    "\n",
    "# 25 models, 0.9929\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
